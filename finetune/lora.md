### LoRA 的秩和缩放系数

#### LoRA 的秩（Rank）
LoRA（Low-Rank Adaptation）通过将原始的全连接层参数矩阵分解为两个低秩矩阵的乘积来减少参数量。这两个低秩矩阵的秩（`r`）是分解矩阵的大小，表示两个矩阵的维度之一。

具体来说，如果原始权重矩阵 `W` 的大小是 `m x n`，则 LoRA 将其分解为两个较小的矩阵 `A` 和 `B`：
- `A` 的大小是 `m x r`
- `B` 的大小是 `r x n`

这里 `r` 就是 LoRA 的秩。通过选择较小的 `r` 值，可以显著减少参数数量。

#### LoRA 的缩放系数（Alpha）
缩放系数（`alpha`）用于调整分解后矩阵的值，以保持数值稳定性。具体来说，LoRA 使用一个缩放因子 `alpha` 将低秩矩阵的乘积缩放：

\[ W' = W + \alpha \cdot (A \cdot B) \]

其中，`W` 是原始权重矩阵，`A` 和 `B` 是低秩分解矩阵，`alpha` 是缩放系数。

缩放系数 `alpha` 的作用是确保分解后的矩阵乘积在数值上与原始矩阵的大小相匹配，以防止在训练过程中出现数值不稳定的问题。

### GLM4 原始参数

为了说明 LoRA 是如何应用于 GLM4 模型，我们需要了解 GLM4 模型的原始参数设置。GLM4 是一种基于 GPT-3 架构的预训练语言模型，以下是其一些典型的超参数：

- **层数（Layers）**：48 层（以 GPT-3 的 175B 模型为例）
- **隐藏层大小（Hidden Size）**：12288
- **注意力头数（Attention Heads）**：96
- **总参数量（Total Parameters）**：1750亿（175B）

GLM4 模型的具体参数量可能会有所不同，但一般来说，其规模和 GPT-3 类似。

### 举例说明

假设 GLM4 模型中的一个全连接层权重矩阵 `W` 的大小为 `12288 x 12288`。在应用 LoRA 后：

- 选择秩 `r = 8`
- 缩放系数 `alpha = 32`

则原始权重矩阵 `W` 将被分解为两个较小的矩阵 `A` 和 `B`：

- `A` 的大小为 `12288 x 8`
- `B` 的大小为 `8 x 12288`

LoRA 的最终权重表示为：

\[ W' = W + 32 \cdot (A \cdot B) \]

通过这种方式，LoRA 大幅减少了需要微调的参数数量，从而提升了训练效率，同时保持了模型的性能。

### 总结

- **LoRA 的秩（Rank）**：表示分解后矩阵的大小，通过较小的秩来减少参数数量。
- **LoRA 的缩放系数（Alpha）**：用于调整低秩矩阵乘积的值，确保数值稳定性。
- **GLM4 原始参数**：GLM4 模型的规模与 GPT-3 类似，具有数十亿到数千亿的参数。

LoRA 技术通过低秩分解和缩放系数的结合，提供了一种高效的方式来微调大规模预训练模型，从而在保持模型性能的同时显著减少计算资源的需求。